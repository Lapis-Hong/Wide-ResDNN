# Model Parameter Configuration

# Wide Parameters
wide_learning_rate: 0.1
wide_lr_decay: true
# regularization parameters, optional
wide_l1: 0.5
wide_l2: 1


# Deep Parameters
# hidden_units: List of each hidden layer units, set nested list for Multi DNN.
# connect_mode: one of {`normal`, `first_dense`, `last_dense`, `dense`, `resnet`} or arbitrary connections
#   1. `normal`: normal DNN with no residual connections.
#   2. `first_dense`: add addition connections from first input layer to all hidden layers.
#   3. `last_dense`: add addition connections from all previous layers to last layer.
#   4. `dense`: add addition connections between all layers, similar to DenseNet.
#   5. `resnet`: add addition connections between adjacent layers, similar to ResNet.
#   6. arbitrary connections list: add addition connections from layer_0 to layer_1 like 0-1.
#      eg: [0-1,0-3,1-2]  index start from zero(input_layer), max index is len(hidden_units), smaller index first.
# residual_mode: one of `add` or `concat`, must be set when connect_mode is not `normal`

# To use multi dnn model, set nested hidden_units, list connect_mode, list residual_mode
# Examples:
# hidden_units: [[1024, 12,256], [512,256]]
# connect_mode: [normal, dense]
# residual_mode: [add, concat]

# network architecture
hidden_units: [1024,512,256]
connect_mode: normal
residual_mode: concat

deep_learning_rate: 0.1
deep_lr_decay: false
activation_function: tf.nn.relu
# regularization parameters, optional, set empty to be default None
deep_l1: 0.01
deep_l2: 0.01
dropout:
batch_normalization: true




